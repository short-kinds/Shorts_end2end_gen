"""
ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§ ë° ë³¸ë¬¸ ì¶”ì¶œ
"""

import os
import re
import time
import random
import unicodedata
import requests
import json
from typing import List, Dict, Optional, Tuple
from urllib.parse import urlparse, parse_qs, urlunparse
from bs4 import BeautifulSoup, UnicodeDammit
from dateutil import parser
from config import Config

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì„¤ì • ë° ìƒìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SAVE_ONLY_PROVIDERS = set(Config.ISSUE_PROVIDERS_FILTER)

NOISY_PATTERNS = re.compile(
    r"(ad|ads|advert|sponsor|banner|promo|related|recommend|rec-|"
    r"share|social|subscribe|footer|copyright|notice|policy|"
    r"widget|sidebar|nav|breadcrumb|comment|emoji|btn|login|"
    r"headline_list|breaking|hot|most|popular)", re.I
)

BAN_SUBSTRINGS = [
    "all rights reserved", "ë¬´ë‹¨ ì „ìž¬", "ë¬´ë‹¨ì „ìž¬", "ìž¬ë°°í¬ ê¸ˆì§€", "aií•™ìŠµ ì´ìš© ê¸ˆì§€",
    "ì €ìž‘ê¶Œ", "copyright",
    "êµ¬ë…", "ì•±ì—ì„œ ë³´ê¸°", "ì•±ì—ì„œë§Œ", "ë‰´ìŠ¤ë ˆí„°", "ì•Œë¦¼ ì„¤ì •", "í”„ë¦¬ë¯¸ì—„", "ìœ ë£ŒíšŒì›",
    "ê³µìœ í•˜ê¸°", "íŽ˜ì´ìŠ¤ë¶", "íŠ¸ìœ„í„°", "ì¹´ì¹´ì˜¤", "ì¸ìŠ¤íƒ€ê·¸ëž¨", "ìœ íŠœë¸Œ",
    "ì†ë³´", "ë§Žì´ ë³¸", "ì¶”ì²œ ê¸°ì‚¬", "ì—°ê´€ ê¸°ì‚¬", "ê´€ë ¨ ê¸°ì‚¬",
    "ë¬¸ì˜:", "ë¬¸ì˜ â€§ ì œë³´", "ì œë³´:", "ê´‘ê³  ë¬¸ì˜", "í›„ì›í•˜ê¸°",
    "ì‚¬ì§„=", "ì˜ìƒ="
]

DOMAIN_SELECTORS = {
    # êµ­ë¯¼ì¼ë³´
    "www.kmib.co.kr": [
        "#articleBody", "div#articleBody", ".article-body", ".art_body", ".news_view", "article",
        "[itemprop='articleBody']"
    ],
    "m.kmib.co.kr": [
        "#articleBody", ".article-body", ".art_body", ".news_view", "article",
        "[itemprop='articleBody']"
    ],
    "amp.kmib.co.kr": [
        "[itemprop='articleBody']", "article", "#articleBody"
    ],
    # SBS
    "news.sbs.co.kr": [
        "[itemprop='articleBody']", "article", "#news_body_area", ".article_cont", ".news_cnt",
        ".news_text", ".text_area", ".viewer"
    ],
    # KBS
    "news.kbs.co.kr": [
        "[itemprop='articleBody']",
        "article .detail-body", "article .content", "article",
        "#cont_newstext", ".detailContent", ".detail_body",
        "#news_textArea", "#newsContent", ".news_body", "#content", "#contents"
    ],
    # MBC
    "imnews.imbc.com": [
        ".news_txt", "[itemprop='articleBody']", "article .news_txt",
    ],
    # ì¡°ì„ 
    "www.chosun.com": [
        "[itemprop='articleBody']", "article .article-body", "article .content", "article",
        "div#news_item", "div#content", "div#contents", ".article-body__content"
    ],
    "news.chosun.com": [
        "[itemprop='articleBody']", "article", "#news_body_id", ".article-body", ".par", "#news_content"
    ],
    "amp.chosun.com": [
        "[itemprop='articleBody']", "article", ".article-body", ".content", "#content"
    ],
    # ì¤‘ì•™
    "www.joongang.co.kr": [
        "[itemprop='articleBody']", "article .article-body", "article .content", "article",
        "#article_body", ".article-body", ".ab_sub",
    ],
    "news.joins.com": [
        "[itemprop='articleBody']", "article", "#article_body", ".article_body", ".content"
    ],
    # ë™ì•„
    "www.donga.com": [
        "[itemprop='articleBody']", "article .article_body", "article .article_txt", "article",
        "#content", ".content", ".article_txt", ".article_body"
    ],
    "news.donga.com": [
        "[itemprop='articleBody']", "article .article_txt", "article .article_body", "article",
        "#content", ".content"
    ],
    # í•œê²¨ë ˆ
    "www.hani.co.kr": [
        "[itemprop='articleBody']", "article .article-text", "article .text", "article",
        ".article-text", ".text", "#contents", "#content"
    ],
    "m.hani.co.kr": [
        "[itemprop='articleBody']", "article .text", "article", ".text", ".article-text", "#content", "#contents"
    ],
    # ê²½í–¥
    "www.khan.co.kr": [
        "[itemprop='articleBody']", "article .article-body", "article .article_txt", "article",
        "#articleBody", ".art_body", ".scroll-article", "#article", ".article_txt",
        "article[role='article']", ".amp-article-body", "main article", "main .article-body", "#contents"
    ],
    "m.khan.co.kr": [
        "[itemprop='articleBody']", "#articleBody", ".article-body", "article",
        ".art_body", ".article_txt", "#content"
    ],
}

EMAIL_RE = re.compile(r"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}")
URL_RE = re.compile(r"https?://\S+")
MULTIDASH_RE = re.compile(r"[-â€“â€”]{3,}")
SENTENCE_END_RE = re.compile(r"[\.!?â€¦]|[ë‹¤ìš”ì£ ]\s*$|["']\s*$")

REQ_HEADERS = {
    "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) "
                   "Chrome/122.0.0.0 Safari/537.36"),
    "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8"
}

DENY_FALLBACK_HOSTS = set()

KINDS_HEADERS = {"Content-Type": "application/json"}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def clean_date(raw_date: str) -> str:
    """ISO8601 ë‚ ì§œë¥¼ YYYY-MM-DDë¡œ ë³€í™˜"""
    try:
        dt = parser.parse(raw_date)
        return dt.strftime("%Y-%m-%d")
    except Exception:
        return raw_date

def normalize_text(s: str) -> str:
    if not s:
        return ""
    s = unicodedata.normalize("NFKC", s)
    s = s.replace("\r\n", "\n")
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{3,}", "\n\n", s.strip())
    return s

def safe_filename(name: str, maxlen: int = 80) -> str:
    name = name or "article"
    name = re.sub(r"[^\wê°€-íž£\-_. ]", "_", name)
    return name[:maxlen].strip() or "article"

def _line_is_noise(line: str) -> bool:
    s = line.strip()
    if not s:
        return True
    low = s.lower()
    if EMAIL_RE.search(s) and len(s) < 80:
        return True
    if URL_RE.search(s) and (len(s) < 80 or (sum(len(x) for x in URL_RE.findall(s))/max(1,len(s)) > 0.3)):
        return True
    if MULTIDASH_RE.search(s):
        return True
    for bad in BAN_SUBSTRINGS:
        if bad.lower() in low:
            return True
    if len(s) < 15:
        return True
    return False

def _strip_noisy_nodes(soup: BeautifulSoup) -> None:
    for tag in soup.select("script, style, noscript, iframe, form, aside, nav, header, footer"):
        try:
            tag.decompose()
        except Exception:
            pass
    for node in list(soup.find_all(True)):
        try:
            node_id = node.get("id") or ""
            cls = node.get("class") or []
            role = (node.get("role") or "").lower()
            id_ok = bool(NOISY_PATTERNS.search(str(node_id)))
            cls_ok = any(NOISY_PATTERNS.search(str(c)) for c in cls)
            role_ok = role in {"banner", "complementary", "navigation"}
            text_len = len(node.get_text(strip=True) or "")
            link_len = sum(len(a.get_text(strip=True) or "") for a in node.find_all("a"))
            link_dense = (link_len / max(1, text_len)) if text_len else 0.0
            if id_ok or cls_ok or role_ok or link_dense > 0.5:
                node.decompose()
        except Exception:
            continue

def _get_text_from_container(node: BeautifulSoup) -> str:
    ps = node.find_all("p")
    if ps:
        parts = [p.get_text(" ", strip=True) for p in ps]
        parts = [t for t in parts if t and len(t) > 3]
        return " ".join(parts)
    for br in node.find_all("br"):
        br.replace_with("\n")
    return node.get_text(" ", strip=True)

def post_cleanup_text(text: str) -> str:
    text = text.replace("\xa0", " ").replace("\u200b", "")
    text = re.sub(r"\s+\n", "\n", text)
    lines = [ln.strip() for ln in text.splitlines()]
    kept: List[str] = []
    for ln in lines:
        if _line_is_noise(ln):
            continue
        ln = EMAIL_RE.sub("", ln)
        ln = URL_RE.sub("", ln)
        ln = re.sub(r"\s{2,}", " ", ln).strip()
        if ln:
            kept.append(ln)
    out = "\n\n".join(kept)
    out = re.sub(r"\n{3,}", "\n\n", out).strip()
    return out

def post_cleanup_text_broadcast(text: str) -> str:
    text = text.replace("\xa0", " ").replace("\u200b", "")
    text = re.sub(r"\s+\n", "\n", text)
    lines = []
    for ln in text.splitlines():
        ln = EMAIL_RE.sub("", ln)
        ln = URL_RE.sub("", ln)
        ln = re.sub(r"\s{2,}", " ", ln).strip()
        lines.append(ln)
    out = "\n".join([x for x in lines if x is not None])
    out = re.sub(r"\n{3,}", "\n\n", out).strip()
    return out

def post_cleanup_text_light(text: str) -> str:
    text = text.replace("\xa0", " ").replace("\u200b", "")
    text = re.sub(r"\s+\n", "\n", text)
    lines = []
    for ln in text.splitlines():
        if not ln.strip():
            lines.append("")
            continue
        ln = EMAIL_RE.sub("", ln)
        ln = URL_RE.sub("", ln)
        ln = re.sub(r"\s{2,}", " ", ln).strip()
        lines.append(ln)
    out = "\n".join(lines)
    out = re.sub(r"\n{3,}", "\n\n", out).strip()
    return out


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì œëª© ì¶”ì¶œ & ìœ ì‚¬ë„
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

GENERIC_TITLE_PATTERNS = [
    r"^\s*ê²½í–¥\s*ì‹ ë¬¸\s*$", r"^\s*ê²½í–¥ì‹ ë¬¸\s*$", r"^\s*The\s+Kyunghyang\s+Shinmun\s*$",
    r"^\s*í•œê²¨ë ˆ\s*$", r"^\s*Hankyoreh\s*$",
    r"^\s*SBS\s*ë‰´ìŠ¤\s*$", r"^\s*KBS\s*ë‰´ìŠ¤\s*$", r"^\s*MBC\s*ë‰´ìŠ¤\s*$",
    r"^\s*ì¡°ì„ ì¼ë³´\s*$", r"^\s*ì¤‘ì•™ì¼ë³´\s*$", r"^\s*ë™ì•„ì¼ë³´\s*$",
    r"^\s*êµ­ë¯¼ì¼ë³´\s*$", r"^\s*Chosun\s*Ilbo\s*$", r"^\s*JoongAng\s*Ilbo\s*$",
    r"^\s*Donga\s*Ilbo\s*$",
]
GENERIC_TITLE_RE = re.compile("|".join(GENERIC_TITLE_PATTERNS), re.IGNORECASE)

def is_generic_title(title: str, provider: str) -> bool:
    t = (title or "").strip()
    if not t:
        return True
    if GENERIC_TITLE_RE.match(t):
        return True
    if re.sub(r"\s+", "", t) == re.sub(r"\s+", "", provider or ""):
        return True
    if len(t) < 4:
        return True
    return False

def _extract_title_from_html(html: str) -> str:
    soup = BeautifulSoup(html, "html.parser")
    # JSON-LD headline
    for tag in soup.find_all("script", attrs={"type": "application/ld+json"}):
        try:
            data = tag.string or tag.text
            if not data:
                continue
            jd = json.loads(data)
            objs = [jd] if isinstance(jd, dict) else (jd if isinstance(jd, list) else [])
            if isinstance(jd, dict) and "@graph" in jd and isinstance(jd["@graph"], list):
                objs += jd["@graph"]
            for obj in objs:
                if isinstance(obj, dict):
                    headline = obj.get("headline")
                    if isinstance(headline, str) and headline.strip():
                        return normalize_text(headline)
        except Exception:
            continue
    # twitter:title
    tw = soup.find("meta", attrs={"name": "twitter:title"})
    if tw and tw.get("content"):
        return normalize_text(tw["content"])
    # og:title
    og = soup.find("meta", attrs={"property": "og:title"}) or soup.find("meta", attrs={"name": "og:title"})
    if og and og.get("content"):
        return normalize_text(og["content"])
    # h1
    h1 = soup.find("h1")
    if h1 and h1.get_text(strip=True):
        return normalize_text(h1.get_text(" ", strip=True))
    # <title>
    if soup.title and soup.title.get_text(strip=True):
        return normalize_text(soup.title.get_text(" ", strip=True))
    return ""

def _title_similarity(a: str, b: str) -> float:
    def toks(x: str) -> set:
        x = re.sub(r"[^0-9A-Za-zê°€-íž£ ]", " ", x or "")
        return set([t for t in x.lower().split() if t])
    A, B = toks(a), toks(b)
    if not A or not B:
        return 0.0
    return len(A & B) / len(A | B)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# HTML íŒŒì‹±
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _decode_bytes_safely(content: bytes, headers: dict) -> str:
    ct = (headers.get("Content-Type") or headers.get("content-type") or "").lower()
    if "charset=" in ct:
        enc = ct.split("charset=")[-1].split(";")[0].strip()
        try:
            return content.decode(enc, errors="replace")
        except Exception:
            pass
    dammit = UnicodeDammit(content, is_html=True)
    if dammit.unicode_markup:
        return dammit.unicode_markup
    for enc in ("utf-8", "euc-kr", "cp949", "iso-8859-1"):
        try:
            return content.decode(enc, errors="replace")
        except Exception:
            continue
    return content.decode("utf-8", errors="replace")

def _extract_from_html(html: str, url_hint: str = "") -> Optional[str]:
    soup = BeautifulSoup(html, "html.parser")
    _strip_noisy_nodes(soup)
    host = urlparse(url_hint).netloc.lower()

    def _clean(txt: str) -> str:
        if "imnews.imbc.com" in host or "news.kbs.co.kr" in host:
            return post_cleanup_text_broadcast(txt)
        if "khan.co.kr" in host and "/amp" in url_hint:
            return post_cleanup_text_light(txt)
        return post_cleanup_text(txt)

    # JSON-LD
    try:
        for tag in soup.find_all("script", attrs={"type": "application/ld+json"}):
            data = tag.string or tag.text
            if not data:
                continue
            jd = json.loads(data)
            objs = [jd] if isinstance(jd, dict) else (jd if isinstance(jd, list) else [])
            if isinstance(jd, dict) and "@graph" in jd and isinstance(jd["@graph"], list):
                objs += jd["@graph"]
            for obj in objs:
                if not isinstance(obj, dict):
                    continue
                tp = obj.get("@type") or obj.get("type") or ""
                if isinstance(tp, list):
                    tp = " ".join(tp)
                if "NewsArticle" in tp or "Article" in tp:
                    body = obj.get("articleBody") or obj.get("body")
                    if body and len(body) > 180:
                        return post_cleanup_text(normalize_text(body))
    except Exception:
        pass

    # ë„ë©”ì¸ ì „ìš©
    if host in DOMAIN_SELECTORS:
        for sel in DOMAIN_SELECTORS[host]:
            node = soup.select_one(sel)
            if node and node.get_text(strip=True):
                _strip_noisy_nodes(node)
                txt = _get_text_from_container(node)
                txt = _clean(txt)
                if len(txt) > 180:
                    return txt

    # itemprop
    node = soup.select_one('[itemprop="articleBody"]')
    if node and node.get_text(strip=True):
        _strip_noisy_nodes(node)
        txt = _get_text_from_container(node)
        txt = _clean(txt)
        if len(txt) > 180:
            return txt

    # article
    art = soup.find("article")
    if art and art.get_text(strip=True):
        _strip_noisy_nodes(art)
        txt = _get_text_from_container(art)
        txt = _clean(txt)
        if len(txt) > 180:
            return txt

    # ë²”ìš© í›„ë³´
    for sel in [
        "div#articleBodyContents", "div#newsEndContents",
        "div.article_body", "div#articeBody", "div.article",
        "div#content", "div#contents", ".article-body", ".content"
    ]:
        node = soup.select_one(sel)
        if node and node.get_text(strip=True):
            _strip_noisy_nodes(node)
            txt = _get_text_from_container(node)
            txt = _clean(txt)
            if len(txt) > 180:
                return txt

    # 2ì°¨: html5lib
    try:
        soup_h5 = BeautifulSoup(html, "html5lib")
        _strip_noisy_nodes(soup_h5)

        if host in DOMAIN_SELECTORS:
            for sel in DOMAIN_SELECTORS[host]:
                node = soup_h5.select_one(sel)
                if node and node.get_text(strip=True):
                    _strip_noisy_nodes(node)
                    txt = _get_text_from_container(node)
                    txt = _clean(txt)
                    if len(txt) > 180:
                        return txt

        node = soup_h5.select_one('[itemprop="articleBody"]')
        if node and node.get_text(strip=True):
            _strip_noisy_nodes(node)
            txt = _get_text_from_container(node)
            txt = _clean(txt)
            if len(txt) > 180:
                return txt

        art = soup_h5.find("article")
        if art and art.get_text(strip=True):
            _strip_noisy_nodes(art)
            txt = _get_text_from_container(art)
            txt = _clean(txt)
            if len(txt) > 180:
                return txt
    except Exception:
        pass

    return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# URL ë¦¬ë¼ì´íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def make_rewrite_candidates(url: str) -> List[str]:
    cand = [url]
    try:
        p = urlparse(url)
        host = p.netloc.lower()
        path = p.path
        query = p.query

        def _with(host_new=None, path_new=None, query_new=None):
            return urlunparse((
                p.scheme, host_new or host, path_new if path_new is not None else path,
                p.params, query_new if query_new is not None else query, p.fragment
            ))

        # SBS AMP
        if "news.sbs.co.kr" in host:
            q = parse_qs(query)
            nid = (q.get("news_id") or [None])[0]
            if nid:
                cand.append(f"https://news.sbs.co.kr/amp/news.amp?news_id={nid}")

        # ì¤‘ì•™ AMP
        if "joongang.co.kr" in host or "joins.com" in host:
            q_amp = query + ("&" if query else "") + "view=amp"
            cand.append(_with(query_new=q_amp))

        # KMIB
        if "kmib.co.kr" in host:
            cand.append(_with(host_new="m.kmib.co.kr"))
            cand.append(_with(host_new="news.kmib.co.kr"))
            cand.append(_with(host_new="amp.kmib.co.kr"))
            q_amp = query + ("&" if query else "") + "view=amp"
            cand.append(_with(query_new=q_amp))

        # ì¡°ì„ 
        if "chosun.com" in host:
            if not path.endswith("/amp/"):
                cand.append(_with(path_new=(path.rstrip("/") + "/amp/")))
            cand.append(_with(host_new="news.chosun.com"))

        # ë™ì•„
        if "donga.com" in host:
            if "news.donga.com" in host:
                if "/all/" in path and "/amp/" not in path:
                    cand.append(_with(path_new=path.replace("/all/", "/amp/all/")))
                cand.append(_with(host_new="www.donga.com"))
            else:
                cand.append(_with(host_new="news.donga.com"))
                if "/all/" in path and "/amp/" not in path:
                    cand.append(_with(host_new="news.donga.com", path_new=path.replace("/all/", "/amp/all/")))

        # í•œê²¨ë ˆ
        if "hani.co.kr" in host:
            cand.append(_with(host_new="m.hani.co.kr"))
            q_m = query + ("&" if query else "") + "m=1"
            cand.append(_with(query_new=q_m))

        # ê²½í–¥
        if "khan.co.kr" in host:
            if not path.endswith("/amp"):
                cand.append(_with(path_new=(path.rstrip("/") + "/amp")))
            cand.append(_with(host_new="m.khan.co.kr"))
            q_amp = query + ("&" if query else "") + "output=amp"
            cand.append(_with(query_new=q_amp))

    except Exception:
        pass

    # ì¤‘ë³µ ì œê±°
    uniq, seen = [], set()
    for c in cand:
        if c and c not in seen:
            uniq.append(c); seen.add(c)
    return uniq

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì›ë¬¸ í¬ë¡¤ë§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def crawl_article(url: str, timeout: int = 15) -> Optional[Tuple[str, str]]:
    """ì„±ê³µ ì‹œ (ë³¸ë¬¸, íŽ˜ì´ì§€ì œëª©) ë°˜í™˜"""
    try:
        candidates = make_rewrite_candidates(url)

        for i, u in enumerate(candidates):
            if i > 0:
                print(f"  â†³ REWRITE TRY[{i}]: {u}")
            time.sleep(random.uniform(0.6, 1.2))

            if urlparse(u).netloc.lower() in DENY_FALLBACK_HOSTS:
                print(f"  â†³ FALLBACK DISABLED for host={urlparse(u).netloc}")
                continue

            r = requests.get(u, headers=REQ_HEADERS, timeout=timeout)
            print(f"  â†³ GET {u} status={r.status_code}")
            if r.status_code != 200 or not r.content:
                continue
            html = _decode_bytes_safely(r.content, r.headers)

            # MBC ë°©ì–´
            if "imnews.imbc.com" in urlparse(u).netloc.lower():
                low = html.lower()
                if ("ìš”ì²­í•˜ì‹  íŽ˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" in html) or ("class=\"error\"" in low and "mbc" in low):
                    continue

            body = _extract_from_html(html, url_hint=u)
            if body:
                page_title = _extract_title_from_html(html)
                print(f"  â†³ EXTRACT OK len={len(body)}")
                return body, page_title

            # AMP ìž¬ì‹œë„
            try:
                soup_tmp = BeautifulSoup(html, "html.parser")
                amp = soup_tmp.find("link", rel=lambda v: v and "amphtml" in v.lower())
                if amp and amp.get("href"):
                    amp_url = requests.compat.urljoin(u, amp["href"])
                    print(f"  â†³ AMP TRY: {amp_url}")
                    time.sleep(random.uniform(0.5, 1.0))
                    r2 = requests.get(amp_url, headers=REQ_HEADERS, timeout=timeout)
                    if r2.status_code == 200 and r2.content:
                        html2 = _decode_bytes_safely(r2.content, r2.headers)
                        body2 = _extract_from_html(html2, url_hint=amp_url)
                        if body2:
                            page_title2 = _extract_title_from_html(html2)
                            print(f"  â†³ EXTRACT OK (AMP) len={len(body2)}")
                            return body2, page_title2
            except Exception:
                pass

        return None

    except requests.RequestException as e:
        print(f"  â†³ GET ERROR: {e}")
        return None

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# KINDS API
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def kinds_fetch_news_detail(news_ids: List[str], fields: Optional[List[str]] = None) -> Dict:
    url = "https://tools.kinds.or.kr/search/news"
    argument = {"news_ids": news_ids}
    if fields:
        argument["fields"] = fields
    payload = {"access_key": Config.KINDS_ACCESS_KEY, "argument": argument}
    r = requests.post(url, json=payload, headers=KINDS_HEADERS, timeout=20)
    r.raise_for_status()
    return r.json()

def looks_truncated(text: str) -> bool:
    if not text:
        return True
    if len(text) < Config.MIN_REASONABLE_LEN:
        return True
    tail = text.strip()[-40:]
    if not SENTENCE_END_RE.search(tail):
        return True
    return False

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë©”ì¸: ê¸°ì‚¬ í¬ë¡¤ë§
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def crawl_articles(issue_obj: Dict[str, Any], per_topic_docs: int = 1) -> List[Dict]:
    """
    ì´ìŠˆ ë°ì´í„°ì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ í¬ë¡¤ë§
    
    Args:
        issue_obj: collect_news_issues() ë°˜í™˜ê°’
        per_topic_docs: í† í”½ë‹¹ ìˆ˜ì§‘í•  ê¸°ì‚¬ ê°œìˆ˜
    
    Returns:
        ê¸°ì‚¬ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    print(f"\nðŸ“ ê¸°ì‚¬ í¬ë¡¤ë§ ì‹œìž‘...")
    
    topics = issue_obj.get("topics", [])
    out: List[Dict] = []

    for t in topics:
        topic_name = t.get("topic")
        topic_rank = t.get("topic_rank")
        cluster = (t.get("news_cluster") or [])[:per_topic_docs]
        
        if not cluster:
            continue

        detail = kinds_fetch_news_detail(
            news_ids=cluster,
            fields=["news_id","title","content","content_original","provider",
                   "published_at","provider_link_page","url","link","byline","category"]
        )
        
        docs = (detail.get("return_object", {}) or {}).get("documents", [])
        if not docs:
            continue

        for d in docs:
            title = d.get("title","") or ""
            provider = d.get("provider","") or ""
            
            if SAVE_ONLY_PROVIDERS and provider not in SAVE_ONLY_PROVIDERS:
                print(f"  â†³ SKIP: provider={provider}")
                continue
                
            category = d.get("category","") or ""
            published_at = d.get("published_at","") or ""
            url = d.get("provider_link_page") or d.get("url") or d.get("link") or ""

            # KINDS ë³¸ë¬¸
            body_kinds_raw = d.get("content_original") or d.get("content") or ""
            body_kinds = normalize_text(body_kinds_raw)

            final_body = body_kinds
            final_title = title
            used_fallback = False

            # ìž˜ë¦¼ ê°ì§€ â†’ í´ë°±
            if looks_truncated(final_body):
                if url and urlparse(url).netloc.lower() not in DENY_FALLBACK_HOSTS:
                    crawled = crawl_article(url)
                    if crawled:
                        crawled_body, crawled_title = crawled
                        if len(crawled_body) > len(final_body) or \
                           (len(final_body) <= 250 and len(crawled_body) >= 400):
                            used_fallback = True
                            final_body = normalize_text(crawled_body)

                            sim = _title_similarity(title, crawled_title or "")
                            if sim < 0.35 and is_generic_title(crawled_title or "", provider):
                                print(f"  â†³ SKIP: ì œëª© ë¯¸ìŠ¤ë§¤ì¹˜ & í´ë°± ì œëª©ì´ ë§¤ì²´ëª…")
                                continue
                            
                            if crawled_title and not is_generic_title(crawled_title, provider) and sim < 0.35:
                                final_title = crawled_title

            # í´ë°± ì‹¤íŒ¨ & ë³¸ë¬¸ ì§§ìŒ â†’ ìŠ¤í‚µ
            if not used_fallback and len(final_body) <= Config.MIN_KINDS_LEN_TO_SAVE_IF_NO_FALLBACK:
                print(f"  â†³ SKIP: í´ë°± ì‹¤íŒ¨ & ë³¸ë¬¸ ì§§ìŒ")
                continue

            print(f"[{topic_name}] '{final_title[:30]}...' ({provider}) âœ…")

            out.append({
                "topic": topic_name,
                "topic_rank": topic_rank,
                "news_id": d.get("news_id",""),
                "title": final_title,
                "provider": provider,
                "category": category,
                "published_at": clean_date(published_at),
                "url": url,
                "content": final_body,
                "source": "fallback" if used_fallback else "kinds"
            })

    print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ: {len(out)}ê°œ ê¸°ì‚¬")
    return out

