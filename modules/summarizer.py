"""
ë‰´ìŠ¤ ê¸°ì‚¬ ìš”ì•½ (T5 ê¸°ë°˜)
"""

import re
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from typing import List, Dict
from config import Config

# ëª¨ë¸ ì´ˆê¸°í™” (ì‹±ê¸€í†¤)
_model = None
_tokenizer = None

def get_model():
    global _model, _tokenizer
    if _model is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸ“š ìš”ì•½ ëª¨ë¸ ë¡œë”© ì¤‘... (device: {device})")
        _tokenizer = AutoTokenizer.from_pretrained(Config.SUMMARY_MODEL)
        _model = AutoModelForSeq2SeqLM.from_pretrained(Config.SUMMARY_MODEL).to(device).eval()
    return _model, _tokenizer

def clean(x): 
    x = re.sub(r"\[[^\]]+\]", " ", x)
    x = re.sub(r"\([^)]+\)", " ", x)
    x = re.sub(r"ë¬´ë‹¨ ì „ì¬.*?ê¸ˆì§€", " ", x)
    x = re.sub(r"\s+", " ", x)
    return x.strip()

def postprocess(summary: str) -> str:
    summary = re.sub(r"[ê°€-í£]{2,4}\s?ê¸°ì", "", summary)
    summary = re.sub(r"ì—°í•©ë‰´ìŠ¤", "", summary)
    summary = re.sub(r"\s+", " ", summary)
    return summary.strip()

@torch.inference_mode()
def summarize(text, max_in=1024, max_out=100, min_out=50,
              beams=5, lp=0.8, no_rep=3, rep_penalty=2.0):
    model, tokenizer = get_model()
    device = next(model.parameters()).device
    
    text = clean(text)
    inputs = tokenizer([text], truncation=True, max_length=max_in, return_tensors="pt").to(device)
    ids = model.generate(
        **inputs,
        num_beams=beams,
        max_length=max_out, 
        min_length=min_out,
        length_penalty=lp,
        no_repeat_ngram_size=no_rep,
        repetition_penalty=rep_penalty,
        early_stopping=True
    )
    return tokenizer.decode(ids[0], skip_special_tokens=True)

def chunk_text(text, n=4):
    """í…ìŠ¤íŠ¸ë¥¼ nê°œë¡œ ê· ë“± ë¶„í• """
    sentences = re.split(r"(?<=[.!?])\s+", text)
    k, m = divmod(len(sentences), n)
    chunks, start = [], 0
    for i in range(n):
        end = start + k + (1 if i < m else 0)
        chunks.append(" ".join(sentences[start:end]))
        start = end
    return chunks

def summarize_dynamic(text):
    """í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ìš”ì•½"""
    tokenizer = get_model()[1]
    length = len(tokenizer.tokenize(text))
    
    if length < 100:
        min_out, max_out = 10, 80
    elif length < 300:
        min_out, max_out = 30, 100
    else:
        min_out, max_out = 50, 120
        
    return summarize(text, min_out=min_out, max_out=max_out)

def clean_for_prompt(text: str) -> str:
    """ì´ë¯¸ì§€ í”„ë¡¬í”„íŠ¸ìš© ì•ˆì „ ë¬¸ìì—´"""
    remove_chars = ['"', "'", """, """, "'", "'"]
    for ch in remove_chars:
        text = text.replace(ch, "")
    return text.strip()

def summarize_in_parts(text, parts=4):
    """í…ìŠ¤íŠ¸ë¥¼ íŒŒíŠ¸ë³„ë¡œ ë‚˜ëˆ  ìš”ì•½"""
    sentences = re.split(r"(?<=[.!?])\s+", text)
    
    # íŒŒíŠ¸ ê°œìˆ˜ ìë™ ì¡°ì •
    if len(sentences) < parts:
        parts = max(1, len(sentences))
    
    chunks = chunk_text(text, n=parts)
    summaries = []
    
    for i, chunk in enumerate(chunks, 1):
        if not chunk.strip():
            continue
        summary = summarize_dynamic(chunk)
        summary = clean_for_prompt(summary)
        summary = postprocess(summary)
        summaries.append(f"íŒŒíŠ¸ {i}: {summary}")
    
    return summaries

def summarize_articles(articles: List[Dict]) -> List[Dict]:
    """
    ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ìš”ì•½
    
    Args:
        articles: crawl_articles() ë°˜í™˜ê°’
    
    Returns:
        ìš”ì•½ì´ ì¶”ê°€ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    """
    print(f"\nâœï¸ ê¸°ì‚¬ ìš”ì•½ ì‹œì‘...")
    
    for i, art in enumerate(articles, 1):
        content = art.get("content", "")
        if not content.strip():
            print(f"  [{i}] SKIP: ë³¸ë¬¸ ì—†ìŒ")
            art["summaries"] = []
            continue
        
        print(f"  [{i}] {art.get('title', '')[:40]}...")
        summaries = summarize_in_parts(content, parts=Config.SUMMARY_PARTS)
        art["summaries"] = summaries
        
        for s in summaries:
            print(f"    - {s}")
    
    print(f"âœ… ìš”ì•½ ì™„ë£Œ: {len(articles)}ê°œ ê¸°ì‚¬")
    return articles
